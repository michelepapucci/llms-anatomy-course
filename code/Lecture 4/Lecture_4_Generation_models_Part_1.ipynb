{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0322271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c297e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a431210425486da4d20a62bc682426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5029311fae84ab89fc38a701f855f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb379fd82fa040a78d5e13813fdcbcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302ad3d8247a490f8ee0bebaaedd755c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"sapienzanlp/Minerva-7B-Instruct-v1.0\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sapienzanlp/Minerva-7B-Instruct-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc5a71",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "\n",
    "Nowadays a common way to use LLM is trough prompting. What this means is that thanks to the model high parameterization, and its intstruction fine-tuning, they can be asked directly to solve tasks with the reasonable expectation that they might have the knowledge to solve it. \n",
    "\n",
    "Zero-shot is simply a straight up question of solving a task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "341371af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " user \n",
      "\n",
      "Il sentimento della seguente frase è positivo, negativo o neutro?\n",
      "La pizza era deliziosa ma il servizio era pessimo. assistant \n",
      "\n",
      " Il sentimento della frase è negativo.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Il sentimento della seguente frase è positivo, negativo o neutro?\\nLa pizza era deliziosa ma il servizio era pessimo.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc18a63",
   "metadata": {},
   "source": [
    "## Few Shot Prompting\n",
    "\n",
    "W.r.t. to Zero-shot, in few shot prompting (specifically, in-context few shot) we provide a number of example of how we would like the model to solve the task, and then provide its input for which we expect an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c91ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " user \n",
      "\n",
      "Il sentimento della seguente frase è positivo, negativo o neutro?\n",
      "La pizza era deliziosa ma il servizio era pessimo. assistant \n",
      "\n",
      "Neutro user \n",
      "\n",
      "Il sentimento della seguente frase è positivo, negativo o neutro?\n",
      "La pizza era deliziosa e il servizio era eccellente. assistant \n",
      "\n",
      "Positivo user \n",
      "\n",
      "Il sentimento della seguente frase è positivo, negativo o neutro?\n",
      "La pizza era immangiabile e il servizio era pessimo. assistant \n",
      "\n",
      "Negativo user \n",
      "\n",
      "Il sentimento della seguente frase è positivo, negativo o neutro?\n",
      "La pizza era buona e il servizio era normale. assistant \n",
      "\n",
      "Positivo\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Il sentimento della seguente frase è positivo, negativo o neutro?\\nLa pizza era deliziosa ma il servizio era pessimo.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Neutro\"},\n",
    "    {\"role\": \"user\", \"content\": \"Il sentimento della seguente frase è positivo, negativo o neutro?\\nLa pizza era deliziosa e il servizio era eccellente.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Positivo\"},\n",
    "    {\"role\": \"user\", \"content\": \"Il sentimento della seguente frase è positivo, negativo o neutro?\\nLa pizza era immangiabile e il servizio era pessimo.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Negativo\"},\n",
    "    {\"role\": \"user\", \"content\": \"Il sentimento della seguente frase è positivo, negativo o neutro?\\nLa pizza era buona e il servizio era normale.\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e05f1",
   "metadata": {},
   "source": [
    "## Decoding methods\n",
    "\n",
    "While the model internals stays the same, there are a number of different ways to decode the model logits on the vocabulary, and choosing simply the highest probability token at every step might not always be the best way to decode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a78061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Scrivi una frase su di un Robot che impara a dipingere.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6da42",
   "metadata": {},
   "source": [
    "### Greedy Decoding\n",
    "\n",
    "Greedy decoding selects the **highest probability token** at each step.\n",
    "\n",
    "```python\n",
    "generate(..., do_sample=False)\n",
    "```\n",
    "\n",
    "`do_sample = False` Disables sampling; the model always chooses the most likely next token. There is no randomness: this produces **deterministic** and **repeatable** output. However it may lack creativity or diversity, and can get stuck in repetitive loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddb6d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' user \\n\\nScrivi una frase su di un Robot che impara a dipingere. assistant \\n\\n \"Il robot pittore sta imparando ad esprimere la sua creatività attraverso l\\'arte della pittura, con ogni pennellata che aggiunge un nuovo strato di colore e bellezza alla tela.\"'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_greedy = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False\n",
    ")\n",
    "tokenizer.decode(output_greedy[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b88662",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "\n",
    "Top-k sampling limits the token pool to the **k most likely tokens**, then randomly samples from them.\n",
    "\n",
    "```python\n",
    "generate(..., do_sample=True, top_k=50)\n",
    "```\n",
    "\n",
    "`do_sample=True` enables sampling. \n",
    "\n",
    "`top_k=50` At each step, only the top 50 most likely tokens are considered. One is thend randomly selected based on probabilities.  \n",
    "This means that higher $k \\rightarrow$ more diversity; lower $k \\rightarrow$ more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d41b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' user \\n\\nScrivi una frase su di un Robot che impara a dipingere. assistant \\n\\n Il robot è in grado di sviluppare abilità sempre più avanzate nella pittura, grazie alla sua capacità di apprendere e perfezionare le tecniche di disegno e colore.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_topk = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.9\n",
    ")\n",
    "tokenizer.decode(output_topk[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b4828",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling\n",
    "\n",
    "Top-p sampling dynamically chooses the **smallest set of tokens whose cumulative probability ≥ p**, then samples from them.\n",
    "\n",
    "```python\n",
    "generate(..., do_sample=True, top_p=0.9)\n",
    "```\n",
    "`top_p=0.9`: Includes only tokens that together make up 90% of the probability mass. Again, `do_sample=True` enables sampling.\n",
    "This is more adaptive than top-k: the number of candidate tokens changes with the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79d2d081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' user \\n\\nScrivi una frase su di un Robot che impara a dipingere. assistant \\n\\n \"Il robot sta imparando ad utilizzare pennelli e colori per creare opere d\\'arte uniche ed emozionanti.\"'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_topp = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.9\n",
    ")\n",
    "tokenizer.decode(output_topp[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443fda26",
   "metadata": {},
   "source": [
    "### Temperature Sampling\n",
    "\n",
    "Temperature controls the **sharpness** of the probability distribution used for sampling.\n",
    "\n",
    "```python\n",
    "generate(..., do_sample=True, temperature=1.2)\n",
    "```\n",
    "How does the Temperature affect generation?\n",
    "\n",
    "`temperature=1.0`: Default setting (no change).\n",
    "\n",
    "`temperature > 1.0`: Flattens the distribution, meaning more **randomness**.\n",
    "\n",
    "`temperature < 1.0`: Sharpens the distribution, meaning more **deterministic**.\n",
    "\n",
    "Works best when combined with top_k or top_p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8958d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" user \\n\\nScrivi una frase su di un Robot che impara a dipingere. assistant \\n\\n Un robot sta imparando a dipingere, cercando di catturare l'emozione e la maestria dei grandi artisti attraverso la sua creazione artistica.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_temp = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=1.2\n",
    ")\n",
    "tokenizer.decode(output_temp[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3715b",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "Beam search keeps **multiple candidate sequences (beams)** at each step and expands them in parallel.\n",
    "\n",
    "```python\n",
    "generate(..., num_beams=5, early_stopping=True, no_repeat_ngram_size=2)\n",
    "```\n",
    "\n",
    "`num_beams=5` means that the top 5 beams (sequence candidates) at every decoding step are kept and the auto-regressive generation continues for all of the 5 candidates. \n",
    "\n",
    "`early_stopping=True`: Stops generation when all beams reach an EOS token.\n",
    "\n",
    "`no_repeat_ngram_size=2`: Prevents repeating any 2-gram (like \"the the\").\n",
    "\n",
    "Produces more globally optimal and coherent results than greedy decoding, and similarly, it's deterministic unless combined with `do_sample=True`. In the end the sequence with the best score is chosen:\n",
    "$$Score = \\frac{\\log P(\\text{sequence})}{\\text{sequence length}^{\\text{length penalty}}}$$\n",
    "\n",
    "The score of each beam is the total log-probability of each token in the sequence, normalized by its lenght. \n",
    "\n",
    "The `length_penalty` parameter can be defined in the `generate(...)` method. $1.0$ is the default value, and balances fairly short and long sequences. Higher numbers penalize longer sequences, while lower numbers penalize shorter sequences. A `length_penalty=0` means that the lenght of the sequence it's not taken into account when evaluating the beams scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f636c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' user \\n\\nScrivi una frase su di un Robot che impara a dipingere. assistant \\n\\n \"Il robot pittore sta imparando ad esprimere la sua creatività attraverso l\\'uso di pennelli e colori, creando opere d\\'arte uniche e originali.\"'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_beam = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "tokenizer.decode(output_beam[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22081889",
   "metadata": {},
   "source": [
    "## Perplexity Extraction\n",
    "\n",
    "We'll se now how to extract perplexity of a model generation. Perplexity is defined as:\n",
    "$$Perplexity = exp(-\\frac{1}{N}\\sum^N_{i=1}P(x_i|x_{<i}))$$\n",
    "\n",
    "Where $N$ is the number of tokens, $x_i$ is the token in position $i$. In general we assume that a **lower** perplexity means a better prediction. It quantifies the model uncertainty, mathematically it is the exponentiated average negative log-likelihood of the predicted tokens. \n",
    "But what does this mean? We take the logarithm of the predicted probabilities (which makes sums possible), compute their average over the sequence, and then apply the exponential to return to a probability-like scale. This yields a number that can be interpreted as the **effective number of equally likely choices** the model had per token.\n",
    "\n",
    "So, for example, a perplexity of 10 means that—on average—the model is as uncertain as if it had to choose uniformly among 10 equally likely tokens at each step. Meaning that, from the model perspective, at each generation step it could chose at random between 10 tokens of its vocabulary. \n",
    "\n",
    "While useful, perplexity has limitations: it only measures *token-level probability accuracy*, not semantic coherence, relevance, or factual correctness. A model can have low perplexity while still generating irrelevant or nonsensical outputs. It also tends to penalize rare but valid completions, which can discourage creative or diverse generation in open-ended tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb2f8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continua la seguente frase: 'Il robot si guardò intorno e vide\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73b29b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    return_dict_in_generate=True, # to return scores\n",
    "    output_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f6f3bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:  user \n",
      "\n",
      "Continua la seguente frase: 'Il robot si guardò intorno e vide assistant \n",
      "\n",
      " che c'era un altro robot vicino a lui.' has a Perplexity of 2.20\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "generated_ids = outputs.sequences[0][inputs[\"input_ids\"].shape[1]:]  # generated tokens only\n",
    "scores = outputs.scores  # list of logits (one tensor per step)\n",
    "\n",
    "# Compute log probs for each generated token\n",
    "log_probs = []\n",
    "for i, score in enumerate(scores):\n",
    "    logits = score[0]                     # shape: [vocab_size]\n",
    "    probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    token_id = generated_ids[i]\n",
    "    log_probs.append(probs[token_id].item())\n",
    "\n",
    "# Average NLL and perplexity\n",
    "nll = -sum(log_probs) / len(log_probs)\n",
    "perplexity = math.exp(nll)\n",
    "print(f\"Decoded text: {decoded_text} has a Perplexity of {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "875e8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=2.5,\n",
    "    return_dict_in_generate=True, # to return scores\n",
    "    output_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f21d7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:  user \n",
      "\n",
      "Continua la seguente frase: 'Il robot si guardò intorno e vide assistant \n",
      "\n",
      " nessun uomo presente.' Ciò implica l'inanimata non presenza ma la condizione potrebbe essere temporaneamente sostituita inserendo nella lista un attore chiave se richiesto e gradito dalle diverse condizioni operative nel quadro contestativo previsto nei dettagli dal compito. Naturalmente i parametri vanno discre has a Perplexity of 41.55\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "generated_ids = outputs.sequences[0][inputs[\"input_ids\"].shape[1]:]  # generated tokens only\n",
    "scores = outputs.scores  # list of logits (one tensor per step)\n",
    "\n",
    "# Compute log probs for each generated token\n",
    "log_probs = []\n",
    "for i, score in enumerate(scores):\n",
    "    logits = score[0]                     # shape: [vocab_size]\n",
    "    probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    token_id = generated_ids[i]\n",
    "    log_probs.append(probs[token_id].item())\n",
    "\n",
    "# Average NLL and perplexity\n",
    "nll = -sum(log_probs) / len(log_probs)\n",
    "perplexity = math.exp(nll)\n",
    "print(f\"Decoded text: {decoded_text} has a Perplexity of {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fefc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
